<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" 
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shahid Moosa - Distributed Systems Engineering</title>
    <link>https://shahidster.tech</link>
    <description>Deep dives into distributed databases, data infrastructure, and production systems. Written by a senior distributed-systems engineer.</description>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Jan 2026 07:20:35 GMT</lastBuildDate>
    <atom:link href="https://shahidster.tech/rss.xml" rel="self" type="application/rss+xml"/>
    <image>
      <url>https://shahidster.tech/favicon.ico</url>
      <title>Shahid Moosa - Distributed Systems Engineering</title>
      <link>https://shahidster.tech</link>
    </image>
    
    <item>
      <title>Understanding CAP Theorem in Production</title>
      <link>https://shahidster.tech/blog/cap-theorem-production</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/cap-theorem-production</guid>
      <description>A practical guide to navigating consistency, availability, and partition tolerance trade-offs when architecting distributed databases.</description>
      <content:encoded><![CDATA[Beyond the Textbook The CAP theorem states that a distributed system can provide only two of three guarantees: Consistency, Availability, and Partition Tolerance. Most engineers learn this in school and move on. But applying it in production requires deeper understanding. This post is the foundation for everything that follows in this series. The tradeoffs we explore here—consistency vs. availability, strong vs. eventual—will resurface in every architectural decision we make. The Practical Reali...]]></content:encoded>
      <pubDate>Sat, 01 Nov 2025 00:00:00 GMT</pubDate>
      <category>Fundamentals</category>
      <category>CAP theorem</category>
      <category>distributed systems</category>
      <category>consistency vs availability</category>
      <category>partition tolerance</category>
    </item>
    <item>
      <title>Pragmatic Consistency: When Stronger Isn&apos;t Better</title>
      <link>https://shahidster.tech/blog/pragmatic-consistency</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/pragmatic-consistency</guid>
      <description>The case against defaulting to strict serializability. Mapping business requirements to the lowest viable consistency level for maximum scalability.</description>
      <content:encoded><![CDATA[The Consistency Trap When architects design distributed systems, they often default to the strongest consistency model available. &quot;Better safe than sorry,&quot; they say. But this safety has a cost—often a severe one. In Part 1, we established the fundamental CAP tradeoffs. Now we go deeper: even within a consistency-prioritizing (CP) system, there&apos;s a spectrum of isolation levels. Choosing wisely is the difference between a system that scales and one that crawls. Strict serializability requires coor...]]></content:encoded>
      <pubDate>Mon, 01 Dec 2025 00:00:00 GMT</pubDate>
      <category>Architecture</category>
      <category>consistency levels</category>
      <category>serializability</category>
      <category>isolation levels</category>
      <category>database scalability</category>
    </item>
    <item>
      <title>The Latency Tax of Separated Compute and Storage</title>
      <link>https://shahidster.tech/blog/latency-tax-separated-compute-storage</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/latency-tax-separated-compute-storage</guid>
      <description>A critical analysis of disaggregated storage architectures vs shared-nothing systems. Examining network I/O penalties and why caching layers fail for high-concurrency point lookups.</description>
      <content:encoded><![CDATA[The Promise and Reality of Disaggregation The cloud-native database movement has championed separated compute and storage as the path to infinite scalability. Services like Snowflake, BigQuery, and Aurora have proven this architecture can work brilliantly—for certain workloads. But there&apos;s a cost that often goes unmentioned in the marketing materials: the latency tax. In Part 2, we explored how different consistency levels trade latency for correctness. Disaggregated storage adds another dimensi...]]></content:encoded>
      <pubDate>Thu, 01 Jan 2026 00:00:00 GMT</pubDate>
      <category>Architecture</category>
      <category>disaggregated storage</category>
      <category>compute storage separation</category>
      <category>NVMe latency</category>
      <category>cache coherency</category>
      <category>HTAP architecture</category>
    </item>
    <item>
      <title>Surviving Data Skew in Distributed Joins</title>
      <link>https://shahidster.tech/blog/data-skew-distributed-joins</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/data-skew-distributed-joins</guid>
      <description>How uneven data distribution destroys shuffle join performance. Contrasting broadcast joins vs repartitioning and why query optimizers often miss skew.</description>
      <content:encoded><![CDATA[The Silent Performance Killer You&apos;ve built a distributed query engine. The benchmarks look great. Then you deploy to production, and some queries take 100x longer than expected. The culprit? Data skew. We&apos;ve established that consistency comes at a cost and storage architecture impacts latency. But even with optimal consistency and storage choices, skew can obliterate performance. Skew occurs when data isn&apos;t evenly distributed across partitions. In a shuffle join, one node might process 90% of th...]]></content:encoded>
      <pubDate>Mon, 01 Dec 2025 00:00:00 GMT</pubDate>
      <category>Performance</category>
      <category>data skew</category>
      <category>distributed joins</category>
      <category>shuffle join</category>
      <category>broadcast join</category>
      <category>Spark AQE</category>
    </item>
    <item>
      <title>Non-Blocking DDL is a Myth: Schema Evolution at Scale</title>
      <link>https://shahidster.tech/blog/non-blocking-ddl-myth</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/non-blocking-ddl-myth</guid>
      <description>How lock propagation and metadata sync cause latency spikes even in &apos;online&apos; DDL. Defensive patterns for schema migration using expansion/contraction strategies.</description>
      <content:encoded><![CDATA[The Marketing vs. Reality Every major database now claims to support &quot;online&quot; or &quot;non-blocking&quot; DDL operations. ALTER TABLE without downtime! Add columns without locking! It sounds perfect—until you try it at scale. The truth is more nuanced. While these operations don&apos;t hold exclusive locks for the entire duration, they still cause measurable performance degradation. Understanding *why* is crucial for planning schema migrations. This builds on our exploration of data skew in distributed joins. ...]]></content:encoded>
      <pubDate>Thu, 01 Jan 2026 00:00:00 GMT</pubDate>
      <category>Deep Dive</category>
      <category>online DDL</category>
      <category>schema migration</category>
      <category>metadata locks</category>
      <category>database migration patterns</category>
      <category>expansion contraction</category>
    </item>
    <item>
      <title>Defensive Ingestion: Managing Backpressure in HTAP Systems</title>
      <link>https://shahidster.tech/blog/defensive-ingestion-backpressure-htap</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/defensive-ingestion-backpressure-htap</guid>
      <description>Protecting HTAP systems from ingestion floods. Why Kafka alone isn&apos;t enough and how to implement database-aware flow control.</description>
      <content:encoded><![CDATA[The HTAP Promise and Peril Hybrid Transactional/Analytical Processing (HTAP) systems promise the best of both worlds: real-time analytics on live operational data. No ETL pipelines, no stale data, no separate systems to maintain. But this power comes with a dangerous coupling: when ingestion spikes, analytical queries suffer. When complex analytics run, transactions slow down. Without careful flow control, one workload can starve the other. This is where CAP theorem realities meet operational pr...]]></content:encoded>
      <pubDate>Sat, 01 Nov 2025 00:00:00 GMT</pubDate>
      <category>Operations</category>
      <category>HTAP systems</category>
      <category>backpressure</category>
      <category>Kafka ingestion</category>
      <category>flow control</category>
      <category>resource isolation</category>
    </item>
    <item>
      <title>Query Optimization at Petabyte Scale</title>
      <link>https://shahidster.tech/blog/query-optimization-petabyte-scale</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/query-optimization-petabyte-scale</guid>
      <description>Lessons learned from debugging slow queries across distributed nodes. Execution plans, index strategies, and memory management.</description>
      <content:encoded><![CDATA[When Queries Go Wrong at Scale A query that runs in 100ms on a gigabyte runs in 10 minutes on a petabyte—if you&apos;re lucky. At scale, every inefficiency is amplified. A bad join strategy doesn&apos;t just slow things down; it can crash your cluster. This is where data skew detection becomes critical. Skew turns a linear scaling problem into an exponential one. Reading Execution Plans The execution plan is your map. Learn to read it fluently. Key things to spot: Sequential Scans on Large Tables This is ...]]></content:encoded>
      <pubDate>Wed, 01 Oct 2025 00:00:00 GMT</pubDate>
      <category>Deep Dive</category>
      <category>query optimization</category>
      <category>execution plans</category>
      <category>covering indexes</category>
      <category>partition pruning</category>
      <category>petabyte scale</category>
    </item>
    <item>
      <title>Incident Response for Database Engineers</title>
      <link>https://shahidster.tech/blog/incident-response-database-engineers</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/incident-response-database-engineers</guid>
      <description>A battle-tested playbook for handling production database incidents. From detection to resolution to post-mortem.</description>
      <content:encoded><![CDATA[When the Pager Goes Off It&apos;s 3 AM. Your phone buzzes. &quot;Database latency critical.&quot; Your heart rate spikes. What do you do? This playbook has been refined over dozens of incidents. It won&apos;t make incidents pleasant, but it will make them manageable. Every concept we&apos;ve covered in this series—from CAP tradeoffs to query optimization—converges in incident response. You need all of it, quickly. Phase 1: Assess (First 5 Minutes) Don&apos;t touch anything yet. Gather information. Immediate Questions 1. What...]]></content:encoded>
      <pubDate>Wed, 01 Oct 2025 00:00:00 GMT</pubDate>
      <category>Operations</category>
      <category>incident response</category>
      <category>database postmortem</category>
      <category>production debugging</category>
      <category>runbooks</category>
      <category>on-call</category>
    </item>
    <item>
      <title>Sharding Strategies That Actually Work</title>
      <link>https://shahidster.tech/blog/sharding-strategies-that-work</link>
      <guid isPermaLink="true">https://shahidster.tech/blog/sharding-strategies-that-work</guid>
      <description>Comparing hash, range, and geo-based sharding with real performance benchmarks and migration patterns.</description>
      <content:encoded><![CDATA[Why Shard? When your database hits the limits of a single machine—CPU, memory, storage, or IOPS—you have two choices: scale up (bigger machine) or scale out (more machines). Sharding is how you scale out. But sharding isn&apos;t free. It adds complexity, complicates joins, and can create hotspots. Choose your strategy carefully. This is the capstone of our series. Sharding decisions draw on everything we&apos;ve covered: CAP tradeoffs determine your consistency across shards, data skew patterns inform key...]]></content:encoded>
      <pubDate>Mon, 01 Sep 2025 00:00:00 GMT</pubDate>
      <category>Architecture</category>
      <category>database sharding</category>
      <category>hash sharding</category>
      <category>range sharding</category>
      <category>geo sharding</category>
      <category>resharding migration</category>
    </item>
  </channel>
</rss>